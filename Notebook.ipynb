{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nlp\n",
    "import random\n",
    "\n",
    "\n",
    "def show_history(h):\n",
    "    epochs_trained = len(h.history['loss'])\n",
    "    plt.figure(figsize=(16, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(range(0, epochs_trained), h.history.get('accuracy'), label='Training')\n",
    "    plt.plot(range(0, epochs_trained), h.history.get('val_accuracy'), label='Validation')\n",
    "    plt.ylim([0., 1.])\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(0, epochs_trained), h.history.get('loss'), label='Training')\n",
    "    plt.plot(range(0, epochs_trained), h.history.get('val_loss'), label='Validation')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def show_confusion_matrix(y_true, y_pred, classes):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    sp = plt.subplot(1, 1, 1)\n",
    "    ctx = sp.matshow(cm)\n",
    "    plt.xticks(list(range(0, 6)), labels=classes)\n",
    "    plt.yticks(list(range(0, 6)), labels=classes)\n",
    "    plt.colorbar(ctx)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "print('Using TensorFlow version', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= nlp.load_dataset('emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "test = dataset['test']\n",
    "validation = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweet(data):\n",
    "    texts = data['text']\n",
    "    labels = data['label']\n",
    "    return texts,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts,labels =get_tweet(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer : map each word to a number\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will have 10K most repeated words, and all of the unmapped words will have the '<UNK>' key\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>') \n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts\n",
    "tokenizer.index_word #sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts[0])\n",
    "tokenizer.texts_to_sequences([texts[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## padding and truncating\n",
    "lengths = [len(t.split(' ')) for t in texts] #getting each sentence length\n",
    "plt.hist(lengths)\n",
    "plt.show()\n",
    "#we can see most of the lengths are between 8 to 20 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to make our input have the same length\n",
    "maxlen = 50 #most of our texts are <= than 50 words\n",
    "#we will pad texts with less than 50, and truncate with more than 50\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence(tokenizer,texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    padded = pad_sequences(sequences,truncating='post',padding='post',maxlen=maxlen)\n",
    "    return padded\n",
    "#'post' parameter so the operations of padding and truncating are at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = get_sequence(tokenizer,texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text smaller than 50\n",
    "print(texts[0])\n",
    "print(tokenizer.texts_to_sequences([texts[0]]))\n",
    "padded_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(texts)):\n",
    "    if len(texts[i].split(' ')) > 50:\n",
    "        break\n",
    "\n",
    "print(tokenizer.texts_to_sequences([texts[i]])\n",
    "      ,'\\n Length = ',len(tokenizer.texts_to_sequences([texts[i]])[0]))\n",
    "padded_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoding labels\n",
    "classes = set(labels)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(labels,bins=11)\n",
    "plt.show\n",
    "#data imbalance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_index = dict((c,i) for i,c in enumerate(classes))\n",
    "index_to_class =dict((v,k) for k,v in class_to_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_ids = lambda labels : np.array([class_to_index.get(x) for x in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = names_to_ids(labels)\n",
    "print(labels[0])\n",
    "train_labels[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling \n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding,Bidirectional,LSTM,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Embedding(10000,16,input_length=maxlen),\n",
    "    Bidirectional(LSTM(20,return_sequences=True)),\n",
    "    Bidirectional(LSTM(20)),\n",
    "    Dense(6,activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() #10k are our tokenized words, 6 dense we got 6 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training the model\n",
    "val_texts,val_labels = get_tweet(validation)\n",
    "val_seq = get_sequence(tokenizer,val_texts)\n",
    "val_labels = names_to_ids(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation[0])\n",
    "print(val_texts[0],val_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = model.fit(\n",
    "padded_train, train_labels,\n",
    "    validation_data=(val_seq,val_labels),\n",
    "    epochs=20,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "show_history(trained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts,test_labels = get_tweet(test)\n",
    "test_seq = get_sequence(tokenizer,test_texts)\n",
    "test_labels = names_to_ids(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.evaluate(test_seq,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sentence : ',test_texts[0])\n",
    "print('Emotion : ',index_to_class[test_labels[0]])\n",
    "p = model.predict(np.expand_dims(test_seq[0],axis=0))[0]\n",
    "pred_class = index_to_class[np.argmax(p).astype('uint8')]\n",
    "print('Predicted :', pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = model.predict(test_seq)\n",
    "print('Sentence : ',test_texts[0])\n",
    "print('Emotion : ',index_to_class[test_labels[0]])\n",
    "index_to_class[np.argmax(p[0])]\n",
    "c = 0\n",
    "for i in range(len(test_seq)):\n",
    "    if index_to_class[test_labels[i]] == index_to_class[np.argmax(p[i])] :\n",
    "        c+=1\n",
    "print(c,len(test_seq), c/len(test_seq))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = np.argmax(model.predict(test_seq),axis=-1)\n",
    "show_confusion_matrix(test_labels,p,list(classes))\n",
    "\n",
    "# we have some issues in the surprise/fear"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
